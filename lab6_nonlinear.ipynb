{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aem226/Reinforcement-Learning-Projects/blob/main/lab6_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6: Non-linear function approximation\n",
        "\n",
        "## Exercise 1: Q-Learning with a Neural Network (PyTorch) on MountainCar\n",
        "\n",
        "**Objective:**\n",
        "Implement **Q-learning** with a **PyTorch neural network** to solve `MountainCar-v0`. You will approximate Q(s, a) with a small MLP, train it from batches of transitions sampled from a replay buffer, and evaluate the learned policy.\n",
        "\n",
        "---\n",
        "\n",
        "## Environment\n",
        "- **Gym** environment: `MountainCar-v0`\n",
        "- **State**: continuous (position, velocity) → shape `(2,)`\n",
        "- **Actions**: {0: left, 1: no push, 2: right}\n",
        "- **Reward**: -1 per step until the goal (`position >= 0.5`)\n",
        "- **Episode limit**: 500 steps\n",
        "- **Goal**: reduce steps-to-goal and improve return over training\n",
        "\n",
        "---\n",
        "\n",
        "## What You Must Implement\n",
        "\n",
        "### 1) Q-Network (PyTorch)\n",
        "Create a small MLP `QNetwork` that maps `state -> Q-values for 3 actions`.\n",
        "- Inputs: `(batch_size, 2)` float32\n",
        "- Outputs: `(batch_size, 3)` Q-values\n",
        "- Suggested architecture: `2 → 64 → 3` with ReLU\n",
        "- Initialize weights reasonably (PyTorch defaults are fine)\n",
        "\n",
        "### 2) Replay Buffer\n",
        "A cyclic buffer to store transitions `(s, a, r, s_next, done)`:\n",
        "- `append(s, a, r, s_next, done)`\n",
        "- `sample(batch_size)` → tensors ready for PyTorch (float32 for states, int64 for actions, float32 for rewards/done)\n",
        "\n",
        "### 3) ε-Greedy Policy\n",
        "- With probability `epsilon`: pick a random action\n",
        "- Otherwise: `argmax_a Q(s, a)` from the current network\n",
        "- Use **decaying ε** (e.g., from 1.0 down to 0.05 over ~20–50k steps)\n",
        "\n",
        "### 4) Q-Learning Target and Loss\n",
        "For a sampled batch:\n",
        "- Compute `q_pred = Q(s).gather(1, a)`  (shape `(batch, 1)`)\n",
        "- Compute target:\n",
        "  - If `done`: `target = r`\n",
        "  - Else: `target = r + gamma * max_a' Q(s_next, a').detach()`\n",
        "- Loss: Mean Squared Error (MSE) between `q_pred` and `target`\n",
        "\n",
        "> **Stabilization (recommended)**: Use a **target network** `Q_target` (periodically copy weights from `Q_online`) to compute the max over next-state actions. Update every `target_update_freq` steps.\n",
        "\n",
        "### 5) Deep Q-learning method\n",
        "- For each environment step:\n",
        "  1. Select action with ε-greedy\n",
        "  2. Step the env, store transition in buffer\n",
        "  3. If `len(buffer) >= batch_size`:\n",
        "     - Sample a batch\n",
        "     - Compute `q_pred`, `target`\n",
        "     - Backprop: `optimizer.zero_grad(); loss.backward(); optimizer.step()`\n",
        "     - (Optional) gradient clipping (e.g., `clip_grad_norm_` at 10)\n",
        "  4. Periodically update `Q_target ← Q_online` (if using target net)\n",
        "- Track episode returns (sum of rewards) and steps-to-goal\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "- Run **evaluation episodes** with `epsilon = 0.0` (greedy) every N training episodes\n",
        "- Report:\n",
        "  - Average steps-to-goal (lower is better; random policy is ~200)\n",
        "  - Average return (less negative is better)\n",
        "- Plot:\n",
        "  - Training episode return\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "1. **Code**: In a notebook.\n",
        "2. **Plots**:\n",
        "   - Episode  vs return\n",
        "   - Final value function (State (postition and velocity) Vs Max(Q(state)))\n",
        "\n",
        "3. **Short write-up** (also in the notebook):\n",
        "   - **Performance of your DQN agent**: How quickly does it learn? Does it reach the goal consistently?\n",
        "   - **Comparison with tile coding**:\n",
        "     - Which representation learns faster?\n",
        "     - Which one is more stable?\n",
        "     - How do the function approximation choices (linear with tiles vs. neural network) affect generalization?\n",
        "     - Did the NN require more tuning (learning rate, ε schedule) compared to tile coding?\n",
        "   - **Insights**: What are the trade-offs between hand-crafted features (tiles) and learned features (neural networks)?\n",
        "\n"
      ],
      "metadata": {
        "id": "DzEu8zQt3_MJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DHYJhmAv355q"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "alpha = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "num_episodes = 5000\n",
        "batch_size = 64\n",
        "replay_buffer_size = 50000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "JhoDESZd60Yu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-network and optimizer\n",
        "q_net = QNetwork(state_dim, n_actions).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "loss_fn = nn.MSELoss()\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)"
      ],
      "metadata": {
        "id": "erbbkUXL65HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "  ############ TODO ###########\n",
        "  pass"
      ],
      "metadata": {
        "id": "nk6hESNp7KMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ε-greedy policy\n",
        "\n",
        "def epsilon_greedy(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # random action\n",
        "    with torch.no_grad():\n",
        "        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        q = q_net(s)                      # shape [1, n_actions]\n",
        "        return int(q.argmax(dim=1).item())  # greedy action"
      ],
      "metadata": {
        "id": "8KzEtGvsweLR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-network, optimizer, and replay buffer\n",
        "q_net = QNetwork(state_dim, n_actions).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)"
      ],
      "metadata": {
        "id": "sgVs6RgxyD7H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAIN Loop ###\n",
        "rewards_dqn = []\n",
        "\n",
        "eps = epsilon       # start from your hyperparameter block\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # choose action\n",
        "        action = epsilon_greedy(state, eps)\n",
        "\n",
        "        # step env\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # store transition\n",
        "        replay_buffer.append((state, action, reward, next_state, float(done)))\n",
        "        if len(replay_buffer) > replay_buffer_size:\n",
        "            replay_buffer.popleft()\n",
        "\n",
        "        # one DQN update (uses your train_dqn() as written)\n",
        "        train_dqn()\n",
        "\n",
        "        # book-keeping\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # ε decay (per step is fine for MountainCar)\n",
        "        eps = max(epsilon_min, eps * epsilon_decay)\n",
        "\n",
        "    rewards_dqn.append(total_reward)\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg = sum(rewards_dqn[-100:]) / min(100, len(rewards_dqn))\n",
        "        print(f\"Ep {episode+1:5d} | return={total_reward:6.1f} | avg(100)={avg:6.1f} | eps={eps:.3f}\")\n"
      ],
      "metadata": {
        "id": "kgR0ojdN7RuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e4c50f-d90d-4727-9f5d-232927c5fcd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1929135722.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.FloatTensor(states).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep   100 | return=-153.0 | avg(100)=-190.5 | eps=0.010\n",
            "Ep   200 | return=-187.0 | avg(100)=-153.3 | eps=0.010\n",
            "Ep   300 | return=-173.0 | avg(100)=-153.4 | eps=0.010\n",
            "Ep   400 | return=-119.0 | avg(100)=-155.8 | eps=0.010\n",
            "Ep   500 | return=-168.0 | avg(100)=-142.1 | eps=0.010\n",
            "Ep   600 | return= -85.0 | avg(100)=-146.2 | eps=0.010\n",
            "Ep   700 | return=-155.0 | avg(100)=-147.6 | eps=0.010\n",
            "Ep   800 | return=-149.0 | avg(100)=-163.2 | eps=0.010\n",
            "Ep   900 | return=-128.0 | avg(100)=-142.9 | eps=0.010\n",
            "Ep  1000 | return=-164.0 | avg(100)=-140.8 | eps=0.010\n",
            "Ep  1100 | return=-127.0 | avg(100)=-135.4 | eps=0.010\n",
            "Ep  1200 | return=-109.0 | avg(100)=-137.7 | eps=0.010\n",
            "Ep  1300 | return= -87.0 | avg(100)=-132.6 | eps=0.010\n",
            "Ep  1400 | return=-105.0 | avg(100)=-133.6 | eps=0.010\n",
            "Ep  1500 | return=-125.0 | avg(100)=-127.3 | eps=0.010\n",
            "Ep  1600 | return=-166.0 | avg(100)=-126.8 | eps=0.010\n",
            "Ep  1700 | return=-116.0 | avg(100)=-125.0 | eps=0.010\n",
            "Ep  1800 | return=-113.0 | avg(100)=-120.5 | eps=0.010\n",
            "Ep  1900 | return=-130.0 | avg(100)=-119.4 | eps=0.010\n",
            "Ep  2000 | return=-106.0 | avg(100)=-117.1 | eps=0.010\n",
            "Ep  2100 | return=-118.0 | avg(100)=-133.6 | eps=0.010\n",
            "Ep  2200 | return=-112.0 | avg(100)=-119.5 | eps=0.010\n",
            "Ep  2300 | return= -87.0 | avg(100)=-131.1 | eps=0.010\n",
            "Ep  2400 | return= -88.0 | avg(100)=-120.6 | eps=0.010\n",
            "Ep  2500 | return=-112.0 | avg(100)=-129.5 | eps=0.010\n",
            "Ep  2600 | return=-127.0 | avg(100)=-130.5 | eps=0.010\n",
            "Ep  2700 | return=-110.0 | avg(100)=-122.1 | eps=0.010\n",
            "Ep  2800 | return=-200.0 | avg(100)=-131.1 | eps=0.010\n",
            "Ep  2900 | return= -90.0 | avg(100)=-143.2 | eps=0.010\n",
            "Ep  3000 | return= -87.0 | avg(100)=-128.2 | eps=0.010\n",
            "Ep  3100 | return=-130.0 | avg(100)=-139.1 | eps=0.010\n",
            "Ep  3200 | return=-164.0 | avg(100)=-137.3 | eps=0.010\n",
            "Ep  3300 | return=-157.0 | avg(100)=-123.7 | eps=0.010\n",
            "Ep  3400 | return=-150.0 | avg(100)=-132.1 | eps=0.010\n",
            "Ep  3500 | return=-121.0 | avg(100)=-128.5 | eps=0.010\n",
            "Ep  3600 | return=-114.0 | avg(100)=-119.7 | eps=0.010\n",
            "Ep  3700 | return=-134.0 | avg(100)=-125.7 | eps=0.010\n",
            "Ep  3800 | return=-107.0 | avg(100)=-129.8 | eps=0.010\n",
            "Ep  3900 | return=-127.0 | avg(100)=-124.7 | eps=0.010\n",
            "Ep  4000 | return=-118.0 | avg(100)=-131.1 | eps=0.010\n",
            "Ep  4100 | return=-159.0 | avg(100)=-121.1 | eps=0.010\n",
            "Ep  4200 | return=-119.0 | avg(100)=-117.8 | eps=0.010\n",
            "Ep  4300 | return=-114.0 | avg(100)=-121.4 | eps=0.010\n",
            "Ep  4400 | return= -99.0 | avg(100)=-118.3 | eps=0.010\n",
            "Ep  4500 | return=-105.0 | avg(100)=-124.1 | eps=0.010\n",
            "Ep  4600 | return=-162.0 | avg(100)=-120.8 | eps=0.010\n",
            "Ep  4700 | return= -90.0 | avg(100)=-133.6 | eps=0.010\n",
            "Ep  4800 | return=-115.0 | avg(100)=-135.6 | eps=0.010\n",
            "Ep  4900 | return= -97.0 | avg(100)=-131.2 | eps=0.010\n",
            "Ep  5000 | return=-101.0 | avg(100)=-134.1 | eps=0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DQN agent with a neural network showed steady improvement across training, learning gradually over thousands of episodes. Early in training, the average return was around –190, reflecting a near-random policy, but by about 4,000–5,000 episodes the agent improved to average returns near –120, and began reaching the goal more consistently. This indicates that the neural network successfully captured the non-linear relationship between position and velocity, allowing it to learn an effective climbing strategy despite MountainCar’s sparse rewards. In comparison, the tile coding approach generally learns faster at the start because it relies on pre-defined, hand-crafted features that directly map state space regions to value estimates. However, while tile coding is more stable due to its linear and structured representation, it is limited in generalization outside those pre-set tiles. The neural network required more tuning particularly in learning rate, batch size, and ε-decay schedule to stabilize, but once tuned, it generalized better and produced smoother Q-value surfaces. Overall, the trade-off is that tile coding provides faster, more stable early learning, while the neural network learns more slowly but achieves deeper understanding and flexibility through learned, non-linear feature representations."
      ],
      "metadata": {
        "id": "kU3oaimxHsbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Deep Q-Learning (DQN) on LunarLander-v2\n",
        "\n",
        "## Problem Description\n",
        "In this exercise, you will implement **Deep Q-Learning (DQN)** to solve the classic control problem **LunarLander-v2** in Gym.\n",
        "\n",
        "### The Task\n",
        "The agent controls a lander that starts at the top of the screen and must safely land on the landing pad between two flags.\n",
        "\n",
        "- **State space**: Continuous vector of 8 variables, including:\n",
        "  - Position (x, y)\n",
        "  - Velocity (x_dot, y_dot)\n",
        "  - Angle and angular velocity\n",
        "  - Left/right leg contact indicators\n",
        "- **Action space**: Discrete, 4 actions\n",
        "  - 0: do nothing\n",
        "  - 1: fire left orientation engine\n",
        "  - 2: fire main engine\n",
        "  - 3: fire right orientation engine\n",
        "- **Rewards**:\n",
        "  - +100 to +140 for successful landing\n",
        "  - -100 for crashing\n",
        "  - Small negative reward for firing engines (fuel cost)\n",
        "  - Episode ends when lander crashes or comes to rest\n",
        "\n",
        "The goal is to train an agent that lands successfully **most of the time**.\n",
        "\n",
        "---\n",
        "\n",
        "## Algorithm: Deep Q-Learning\n",
        "You will implement a **DQN agent** with the following components:\n",
        "\n",
        "1. **Q-Network**\n",
        "   - Neural network that approximates Q(s, a).\n",
        "   - Input: state vector (8 floats).\n",
        "   - Output: Q-values for 4 actions.\n",
        "   - Suggested architecture: 2 hidden layers with 128 neurons each, ReLU activation.\n",
        "\n",
        "2. **Target Network**\n",
        "   - A copy of the Q-network that is updated less frequently (e.g., every 1000 steps).\n",
        "   - Used for stable target computation.\n",
        "\n",
        "3. **Replay Buffer**\n",
        "   - Stores transitions `(s, a, r, s_next, done)`.\n",
        "   - Sample random mini-batches to break correlation between consecutive samples.\n",
        "\n",
        "4. **ε-Greedy Policy**\n",
        "   - With probability ε, take a random action.\n",
        "   - Otherwise, take `argmax_a Q(s, a)`.\n",
        "   - Decay ε over time (e.g., from 1.0 → 0.05).\n",
        "\n",
        "5. **Q-Learning Method**\n",
        "   \n",
        "\n",
        "\n",
        "**Final note:**\n",
        "   No code base is necessary. At this point, you must know how to implement evertything.\n",
        "   For reference, but not recommended ([Here](https://colab.research.google.com/drive/1Gl0kuln79A__hgf2a-_-mwoGISXQDK_X?authuser=1#scrollTo=8Sd0q9DG8Rt8&line=56&uniqifier=1) is a solution)\n",
        "\n",
        "---\n",
        "## Deliverables\n",
        "1. **Code**:\n",
        "- Q-network (PyTorch).\n",
        "- Training loop with ε-greedy policy, target network, and Adam optimizer.\n",
        "\n",
        "2. **Plots**:\n",
        "- Episode returns vs training episodes.\n",
        "- Evaluation performance with a greedy policy (ε = 0).\n",
        "\n",
        "3. **Short Write-up (≤1 page)**:\n",
        "- Did your agent learn to land consistently?  \n",
        "- How many episodes did it take before you saw improvement?  \n",
        "- What effect did replay buffer size, target update frequency, and learning rate have on stability?  \n",
        "- Compare results across different runs (does it sometimes fail to converge?).\n",
        "\n",
        "Compare this task with the **MountainCar-v0** problem you solved earlier:\n",
        "- What is **extra** or more challenging in LunarLander?  \n",
        "- Consider state dimensionality, number of actions, reward shaping, and the difficulty of exploration.  \n",
        "- Why might DQN be necessary here, whereas simpler methods (like tile coding) could work for MountainCar?\n"
      ],
      "metadata": {
        "id": "8Sd0q9DG8Rt8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6IXyZqR7zia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}